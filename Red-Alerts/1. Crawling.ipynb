{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font color='Black'>Crawling</font></center></h1>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Imports & Installation---------#\n",
    "#pip install selenium\n",
    "#pip install webdriver\n",
    "#pip install lxml\n",
    "#pip install wordcloud\n",
    "#pip install seaborn\n",
    "#pip install matplotlib\n",
    "#pip install python-bidi\n",
    "#pip install sklearn\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "todays_date = date.today() #Get Today'S Date\n",
    "curryear = todays_date.year\n",
    "swipe = curryear - 2014 #Start From Curr Year To 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to use <u>**Selenium**</u> to navigate automaticly in the website we want to crwaling, until we find the table with all the data we looking for on the site.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install()) #install Are Auto-Website\n",
    "driver.implicitly_wait(10) #Wait until everything is upload\n",
    "driver.get('https://www.oref.org.il//12481-he/Pakar.aspx')\n",
    "\n",
    "\n",
    "#Clicks:\n",
    "\n",
    "element = driver.find_element_by_id(\"ah-chooseDates\").click()#Calendar Buttom\n",
    "sleep(0.9)\n",
    "element = driver.find_element_by_id(\"txtDateFrom\").click()#Open Calendar Itself\n",
    "sleep(0.9)\n",
    "element = driver.find_element_by_class_name(\"datepicker-switch\").click()#Switch From Days To Month\n",
    "sleep(0.9)\n",
    "\n",
    "xp1= \"//*[@id='ctl00_Body']/div[6]/div[2]/table/thead/tr/th[1]\"\n",
    "xp2=\"//*[@id='ctl00_Body']/div[6]/div[2]/table/tbody/tr/td/span[7]\"\n",
    "xp3=\"//*[@id='ctl00_Body']/div[6]/div[1]/table/tbody/tr[4]/td[5]\"\n",
    "\n",
    "for x in range(swipe): #Select The Range\n",
    "    element = driver.find_element_by_xpath(xp1).click()\n",
    "    sleep(0.3)\n",
    "\n",
    "element = driver.find_element_by_xpath(xp2).click()#Choose The Month\n",
    "sleep(0.5)\n",
    "element = driver.find_element_by_xpath(xp3).click()#Choose The Day\n",
    "sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have finished navigating the site we are going to use **BeautifulSoup** to Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(10)#Wait Until All The Data In The Table Will Showing Up\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "soup_date = soup(\"li\",attrs={\"class\" : \"alertTableDateCol\"})\n",
    "soup_time = soup(\"li\",attrs={\"class\" : \"alertTableTimeCol\"})\n",
    "soup_zone = soup(\"li\",attrs={\"class\" : \"alertTableCityNameCol\"})\n",
    "\n",
    "\n",
    "date = [ i.get_text().strip() for i in soup_date ]\n",
    "time = [ i.get_text().strip() for i in soup_time ]\n",
    "zone = [ i.get_text().strip() for i in soup_zone ]\n",
    "\n",
    "d = {\"Zone\":zone , \"Date\": date,\"Time\": time}\n",
    "df = pd.DataFrame(data=d)#create a data frame with all the data\n",
    "df.drop(index= df.index[0] , axis=0, inplace=True)\n",
    "df.drop(index=df.index[-1], axis=0, inplace=True)\n",
    "df_copy = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/df.csv\", encoding='utf-8-sig', index=False)#Create a csv File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the cities do not appear in their original names, but their names appear as the names of their regions.\n",
    "\n",
    "So we need to crawl to another website, from there we take the names for each region we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.mivzaklive.co.il/%D7%94%D7%AA%D7%A8%D7%90%D7%AA-%D7%A6%D7%91%D7%A2-%D7%90%D7%93%D7%95%D7%9D-%D7%9E%D7%A1%D7%A4%D7%A8%D7%99-%D7%A4%D7%95%D7%9C%D7%99%D7%92%D7%95%D7%A0%D7%99%D7%9D-%D7%95%D7%96%D7%9E%D7%A0%D7%99-%D7%94\"\n",
    "res = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_zone_time = soup.find_all(attrs={'style':'width: 150px;'})\n",
    "soup_zone_name = soup.find_all(attrs={'style':'width: 234px;'})\n",
    "soup_zone_number = soup.find_all(attrs={'style':'width: 148px;'})\n",
    "\n",
    "zone_name = [ i.get_text().strip() for i in soup_zone_name ]\n",
    "defense_time = [i.get_text().strip() for i in soup_zone_time] \n",
    "zone_number = [i.get_text().strip() for i in soup_zone_number] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2 = {\"Zone Name\" : zone_name , \"Zone Number\": zone_number, \"Defense Time\" : defense_time }\n",
    "df2 = pd.DataFrame(data = dic2)#create a second data frame with all the data\n",
    "df2.drop(index= df2.index[0] , axis=0, inplace=True) #drop the first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"Data/df2.csv\", encoding='utf-8-sig' ,index=False )\n",
    "df.to_csv(\"Data/df.csv\", encoding='utf-8-sig' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a11925700abfa772c17356028b690db4249aa570a9789268328a6f751d9f1b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
